% Encoding: UTF-8

@Article{Christlein2012,
  author   = {Christlein, Vincent and Riess, Christian and Jordan, Johannes and Riess, Corinna and Angelopoulou, Elli},
  journal  = {IEEE Transactions on Information Forensics and Security},
  title    = {An {Evaluation} of {Popular} {Copy}-{Move} {Forgery} {Detection} {Approaches}},
  year     = {2012},
  issn     = {1556-6013, 1556-6021},
  month    = dec,
  note     = {arXiv: 1208.3665},
  number   = {6},
  pages    = {1841--1854},
  volume   = {7},
  abstract = {A copy-move forgery is created by copying and pasting content within the same image, and potentially post-processing it. In recent years, the detection of copy-move forgeries has become one of the most actively researched topics in blind image forensics. A considerable number of different algorithms have been proposed focusing on different types of postprocessed copies. In this paper, we aim to answer which copy-move forgery detection algorithms and processing steps (e.g., matching, filtering, outlier detection, affine transformation estimation) perform best in various postprocessing scenarios. The focus of our analysis is to evaluate the performance of previously proposed feature sets. We achieve this by casting existing algorithms in a common pipeline. In this paper, we examined the 15 most prominent feature sets. We analyzed the detection performance on a per-image basis and on a per-pixel basis. We created a challenging real-world copy-move dataset, and a software framework for systematic image manipulation. Experiments show, that the keypoint-based features SIFT and SURF, as well as the block-based DCT, DWT, KPCA, PCA and Zernike features perform very well. These feature sets exhibit the best robustness against various noise sources and downsampling, while reliably identifying the copied regions.},
  annote   = {Comment: Main paper: 14 pages, supplemental material: 12 pages, main paper appeared in IEEE Transaction on Information Forensics and Security},
  doi      = {10.1109/TIFS.2012.2218597},
  file     = {:Christlein2012 - An Evaluation of Popular Copy Move Forgery Detection Approaches.pdf:PDF},
  groups   = {Project Base, Hard},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.9},
  url      = {http://arxiv.org/abs/1208.3665},
  urldate  = {2021-09-10},
}

@InProceedings{Trosten2019,
  author    = {Trosten, Daniel J. and Sharma, Puneet},
  booktitle = {Image {Analysis}},
  title     = {Unsupervised {Feature} {Extraction} – {A} {CNN}-{Based} {Approach}},
  year      = {2019},
  address   = {Cham},
  editor    = {Felsberg, Michael and Forssén, Per-Erik and Sintorn, Ida-Maria and Unger, Jonas},
  pages     = {197--208},
  publisher = {Springer International Publishing},
  series    = {Lecture {Notes} in {Computer} {Science}},
  abstract  = {Working with large quantities of digital images can often lead to prohibitive computational challenges due to their massive number of pixels and high dimensionality. The extraction of compressed vectorial representations from images is therefore a task of vital importance in the field of computer vision. In this paper, we propose a new architecture for extracting such features from images in an unsupervised manner, which is based on convolutional neural networks. The model is referred to as the Unsupervised Convolutional Siamese Network (UCSN), and is trained to embed a set of images in a vector space, such that local distance structure in the space of images is approximately preserved. We compare the UCSN to several classical methods by using the extracted features as input to a classification system. Our results indicate that the UCSN produces vectorial representations that are suitable for classification purposes.},
  doi       = {10.1007/978-3-030-20205-7_17},
  file      = {:article.pdf:PDF},
  groups    = {Project Base},
  isbn      = {9783030202057},
  language  = {en},
}

@InProceedings{Kumar2018,
  author    = {Kumar, Abhay and Jain, Nishant and Singh, Chirag and Tripathi, Suraj},
  booktitle = {2018 15th {IEEE} {India} {Council} {International} {Conference} ({INDICON})},
  title     = {Exploiting {SIFT} {Descriptor} for {Rotation} {Invariant} {Convolutional} {Neural} {Network}},
  year      = {2018},
  month     = dec,
  note      = {ISSN: 2325-9418},
  pages     = {1--5},
  abstract  = {This paper presents a novel approach to exploit the distinctive invariant features in convolutional neural network. The proposed CNN model uses Scale Invariant Feature Transform (SIFT) descriptor instead of the max-pooling layer. Max-pooling layer discards the pose, i.e., translational and rotational relationship between the low-level features, and hence unable to capture the spatial hierarchies between low and high level features. The SIFT descriptor layer captures the orientation and the spatial relationship of the features extracted by convolutional layer. The proposed SIFT Descriptor CNN, therefore, combines the feature extraction capabilities of CNN model and rotation invariance of SIFT descriptor. Experimental results on the MNIST and fashionMNIST datasets indicate reasonable improvements over conventional methods available in literature.},
  doi       = {10.1109/INDICON45594.2018.8987153},
  file      = {:1904.00197.pdf:PDF},
  groups    = {Project Base, CNN, Models},
  issn      = {2325-9418},
  keywords  = {Convolutional Neural Network, Max-pooling, SIFT Descriptor, Scale Invariant Feature Transform},
}

@Article{Yang2020,
  author     = {Yang, Chao and Li, Huizhou and Lin, Fangting and Jiang, Bin and Zhao, Hao},
  journal    = {arXiv:1911.08217 [cs]},
  title      = {Constrained {R}-{CNN}: {A} general image manipulation detection model},
  year       = {2020},
  month      = mar,
  note       = {arXiv: 1911.08217},
  abstract   = {Recently, deep learning-based models have exhibited remarkable performance for image manipulation detection. However, most of them suffer from poor universality of handcrafted or predetermined features. Meanwhile, they only focus on manipulation localization and overlook manipulation classification. To address these issues, we propose a coarse-to-fine architecture named Constrained R-CNN for complete and accurate image forensics. First, the learnable manipulation feature extractor learns a unified feature representation directly from data. Second, the attention region proposal network effectively discriminates manipulated regions for the next manipulation classification and coarse localization. Then, the skip structure fuses low-level and high-level information to refine the global manipulation features. Finally, the coarse localization information guides the model to further learn the finer local features and segment out the tampered region. Experimental results show that our model achieves state-of-the-art performance. Especially, the F1 score is increased by 28.4\%, 73.2\%, 13.3\% on the NIST16, COVERAGE, and Columbia dataset.},
  annote     = {Comment: Accepted to IEEE International Conference on Multimedia and Expo (ICME2020)},
  file       = {:Yang2020 - Constrained R CNN_ a General Image Manipulation Detection Model.pdf:PDF},
  groups     = {Project Base, CNN},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
  shorttitle = {Constrained {R}-{CNN}},
  url        = {http://arxiv.org/abs/1911.08217},
  urldate    = {2021-08-05},
}

@Article{RodriguezOrtega2021,
  author    = {Rodriguez-Ortega, Yohanna and Ballesteros, Dora M. and Renza, Diego},
  journal   = {Journal of Imaging},
  title     = {Copy-{Move} {Forgery} {Detection} ({CMFD}) {Using} {Deep} {Learning} for {Image} and {Video} {Forensics}},
  year      = {2021},
  month     = mar,
  number    = {3},
  pages     = {59},
  volume    = {7},
  abstract  = {With the exponential growth of high-quality fake images in social networks and media, it is necessary to develop recognition algorithms for this type of content. One of the most common types of image and video editing consists of duplicating areas of the image, known as the copy-move technique. Traditional image processing approaches manually look for patterns related to the duplicated content, limiting their use in mass data classification. In contrast, approaches based on deep learning have shown better performance and promising results, but they present generalization problems with a high dependence on training data and the need for appropriate selection of hyperparameters. To overcome this, we propose two approaches that use deep learning, a model by a custom architecture and a model by transfer learning. In each case, the impact of the depth of the network is analyzed in terms of precision (P), recall (R) and F1 score. Additionally, the problem of generalization is addressed with images from eight different open access datasets. Finally, the models are compared in terms of evaluation metrics, and training and inference times. The model by transfer learning of VGG-16 achieves metrics about 10\% higher than the model by a custom architecture, however, it requires approximately twice as much inference time as the latter.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  doi       = {10.3390/jimaging7030059},
  file      = {:RodriguezOrtega2021 - Copy Move Forgery Detection (CMFD) Using Deep Learning for Image and Video Forensics.pdf:PDF},
  groups    = {Project Base, CNN},
  keywords  = {copy-move forgery detection, computer vision, deep learning, fake image, transfer learning, VGG},
  language  = {en},
  publisher = {Multidisciplinary Digital Publishing Institute},
  url       = {https://www.mdpi.com/2313-433X/7/3/59},
  urldate   = {2021-08-04},
}

@InProceedings{Wu2018,
  author     = {Wu, Yue and Abd-Almageed, Wael and Natarajan, Prem},
  booktitle  = {Computer {Vision} – {ECCV} 2018},
  title      = {{BusterNet}: {Detecting} {Copy}-{Move} {Image} {Forgery} with {Source}/{Target} {Localization}},
  year       = {2018},
  address    = {Cham},
  editor     = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  pages      = {170--186},
  publisher  = {Springer International Publishing},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {We introduce a novel deep neural architecture for image copy-move forgery detection (CMFD), code-named BusterNet. Unlike previous efforts, BusterNet is a pure, end-to-end trainable, deep neural network solution. It features a two-branch architecture followed by a fusion module. The two branches localize potential manipulation regions via visual artifacts and copy-move regions via visual similarities, respectively. To the best of our knowledge, this is the first CMFD algorithm with discernibility to localize source/target regions. We also propose simple schemes for synthesizing large-scale CMFD samples using out-of-domain datasets, and stage-wise strategies for effective BusterNet training. Our extensive studies demonstrate that BusterNet outperforms state-of-the-art copy-move detection algorithms by a large margin on the two publicly available datasets, CASIA and CoMoFoD, and that it is robust against various known attacks.},
  doi        = {10.1007/978-3-030-01231-1_11},
  file       = {:Rex_Yue_Wu_BusterNet_Detecting_Copy-Move_ECCV_2018_paper.pdf:PDF},
  groups     = {CNN, Models, Applications, Project Base},
  isbn       = {9783030012311},
  keywords   = {Copy-move , Image forgery detection , Deep learning },
  language   = {en},
  readstatus = {skimmed},
  shorttitle = {{BusterNet}},
}

@Conference{Goularas2019,
  author          = {Dionysis Goularas and Sani Kamis},
  title           = {Evaluation of Deep Learning Techniques in Sentiment Analysis from Twitter Data},
  year            = {2019},
  address         = {Istanbul, Turkey},
  pages           = {12--17},
  publisher       = {IEEE},
  abstract        = {This study presents a comparison of different deep learning methods used for sentiment analysis in Twitter data. In this domain, deep learning (DL) techniques, which contribute at the same time to the solution of a wide range of problems, gained popularity among researchers. Particularly, two categories of neural networks are utilized, convolutional neural networks(CNN), which are especially performant in the area of image processing and recurrent neural networks (RNN) which are applied with success in natural language processing (NLP) tasks. In this work we evaluate and compare ensembles and combinations of CNN and a category of RNN the long short-term memory (LSTM) networks. Additionally, we compare different word embedding systems such as the Word2Vec and the global vectors for word representation (GloVe) models. For the evaluation of those methods we used data provided by the international workshop on semantic evaluation (SemEval), which is one of the most popular international workshops on the area. Various tests and combinations are applied and best scoring values for each model are compared in terms of their performance. This study contributes to the field of sentiment analysis by analyzing the performances, advantages and limitations of the above methods with an evaluation procedure under a single testing framework with the same dataset and computing environment.},
  date            = {26-28 Aug. 2019},
  doi             = {10.1109/Deep-ML.2019.00011},
  eventdate       = {26-28 Aug. 2019},
  eventtitleaddon = {Istanbul, Turkey},
  file            = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/Deep-ML.2019.00011.pdf:PDF},
  groups          = {Applications},
  isbn            = {978-1-7281-2915-0},
  journal         = {2019 International Conference on Deep Learning and Machine Learning in Emerging Applications (Deep-ML)},
  keywords        = {Sentiment analysis, Deep learning, Feature extraction, Task analysis, Twitter, Recurrent neural networks, sentiment analysis, deep learning, convolutional neural networks, LSTM, word embedding models, Twitter data},
  priority        = {prio2},
}

@Article{goodfellow2014generative,
  author        = {Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
  title         = {Generative Adversarial Networks},
  year          = {2014},
  abstract      = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model g that captures the data distribution, and a discriminative model d that estimates the probability that a sample came from the training data rather than g. The training procedure for g is to maximize the probability of d making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions g and d, a unique solution exists, with g recovering the training data distribution and d equal to 1 2 everywhere. In the case where g and d are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  eprint        = {1406.2661},
  file          = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/1406.2661.pdf:PDF},
  groups        = {Hard, GAN},
  primaryclass  = {stat.ML},
  priority      = {prio1},
  readstatus    = {skimmed},
}

@Conference{article,
  author   = {Graves, Alex},
  title    = {Generating Sequences With Recurrent Neural Networks},
  year     = {2013},
  month    = {08},
  abstract = {This paper shows how long short-term memory recurrent neural net- works can be used to generate complex sequences with long-range struc- ture, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwrit- ing (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
  file     = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/article - Generating Sequences with Recurrent Neural Networks.pdf:PDF},
  groups   = {Applications, RNN},
  priority = {prio2},
  url      = {https://paperswithcode.com/paper/generating-sequences-with-recurrent-neural},
}

@Article{hinton2015distilling,
  author        = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
  title         = {Distilling the Knowledge in a Neural Network},
  year          = {2015},
  abstract      = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arXiv},
  eprint        = {1503.02531},
  file          = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/44873 - Distilling the Knowledge in a Neural Network.pdf:PDF},
  groups        = {Hard, Models},
  primaryclass  = {stat.ML},
  priority      = {prio2},
  readstatus    = {skimmed},
}

@Conference{CENGIL2018,
  author     = {Emine CENGIL and Ahmet CINAR},
  booktitle  = {2018 International Conference on Artificial Intelligence and Data Processing ({IDAP})},
  title      = {A Deep Learning Based Approach to Lung Cancer Identification},
  year       = {2018},
  month      = {sep},
  publisher  = {{IEEE}},
  abstract   = {Cancer is a very common disease type in worldwide. There are many types of cancer. Lung cancer is the most common type of cancer. Lung cancer which is common in both men and women can be fatal. The initiation of treatment by diagnosing cancer is important in reducing the risk of death. In this paper, classification of lung nodules is performs using ct images of spie-aapm-lungx data. Deep learning has been a popular choice for the classification process in recent years. Especially it is used in the implementation of tensorflow and 3d convolutional neural network architecture from deep learning libraries. 

Index terms—lung cancer, deep learning, resnet, image classification.},
  doi        = {10.1109/idap.2018.8620723},
  file       = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/cengil2018.pdf:PDF},
  groups     = {CNN, Easy, Applications},
  readstatus = {read},
}

@Article{Hussein2019,
  author    = {Sarfaraz Hussein and Pujan Kandel and Candice W. Bolan and Michael B. Wallace and Ulas Bagci},
  journal   = {{IEEE} Transactions on Medical Imaging},
  title     = {Lung and Pancreatic Tumor Characterization in the Deep Learning Era: Novel Supervised and Unsupervised Learning Approaches},
  year      = {2019},
  month     = {aug},
  number    = {8},
  pages     = {1777--1787},
  volume    = {38},
  abstract  = {Risk stratification (characterization) of tumors from radiology images can be more accurate and faster with computeraided diagnosis (cad) tools. Tumor characterization through such tools can also enable non-invasive cancer staging, prognosis, and foster personalized treatment planning as a part of precision medicine. In this study, we propose both supervised and unsupervised machine learning strategies to improve tumor characterization. Our first approach is based on supervised learning for which we demonstrate significant gains with deep learning algorithms, particularly by utilizing a 3d convolutional neural network and transfer learning. Motivated by the radiologists’ interpretations of the scans, we then show how to incorporate task dependent feature representations into a cad system via a graph-regularized sparse multi-task learning (mtl) framework. In the second approach, we explore an unsupervised learning algorithm to address the limited availability of labeled training data, a common problem in medical imaging applications. Inspired by learning from label proportion (llp) approaches in computer vision, we propose to use proportion-svm for characterizing tumors. We also seek the answer to the fundamental question about the goodness of “deep features” for unsupervised tumor classification. We evaluate our proposed supervised and unsupervised learning algorithms on two different tumor diagnosis challenges: lung and pancreas with 1018 ct and 171 mri scans, respectively, and obtain the state-of-the-art sensitivity and specificity results in both problems. 

index terms—unsupervised learning, lung cancer, 3d cnn, ipmn, pancreatic cancer.},
  doi       = {10.1109/tmi.2019.2894349},
  file      = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/TMI.2019.2894349.pdf:PDF},
  groups    = {Easy, CNN, Applications},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Conference{Graves2013,
  author    = {Alex Graves and Abdel-rahman Mohamed and Geoffrey Hinton},
  booktitle = {2013 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
  title     = {Speech recognition with deep recurrent neural networks},
  year      = {2013},
  month     = {may},
  publisher = {{IEEE}},
  abstract  = {Recurrent neural networks (rnns) are a powerful model for sequential data. End-to-end training methods such as connectionist temporal classification make it possible to train rnns for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the long short-term memory rnn architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However rnn performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers rnns. When trained end-to-end with suitable regularisation, we find that deep long short-term memory rnns achieve a test set error of 17.7% on the timit phoneme recognition benchmark, which to our knowledge is the best recorded score. 

Index terms— recurrent neural networks, deep neural networks, speech recognition},
  doi       = {10.1109/icassp.2013.6638947},
  file      = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/1303.5778.pdf:PDF},
  groups    = {RNN, Applications},
}

@Conference{Lee2019,
  author     = {Kyu Beom Lee and Hyu Soung Shin},
  booktitle  = {2019 International Conference on Deep Learning and Machine Learning in Emerging Applications (Deep-{ML})},
  title      = {An Application of a Deep Learning Algorithm for Automatic Detection of Unexpected Accidents Under Bad {CCTV} Monitoring Conditions in Tunnels},
  year       = {2019},
  month      = {aug},
  publisher  = {{IEEE}},
  abstract   = {In this paper, object detection and tracking system (odts) in combination with a well-known deep learning network, faster regional convolution neural network (faster r-cnn), for object detection and conventional object tracking algorithm will be introduced and applied for automatic detection and monitoring of unexpected events on cctvs in tunnels, which are likely to (1) wrong-way driving (wwd), (2) stop, (3) person out of vehicle in tunnel (4) fire. Odts accepts a video frame in time as an input to obtain bounding box (bbox) results by object detection and compares the bboxs of the current and previous video frames to assign a unique id number to each moving and detected object. This system makes it possible to track a moving object in time, which is not usual to be achieved in conventional object detection frameworks. A deep learning model in odts was trained with a dataset of event images in tunnels to average precision (ap) values of 0.8479, 0.7161 and 0.9085 for target objects: car, person, and fire, respectively. Then, based on trained deep learning model, the odts based tunnel cctv accident detection system was tested using four accident videos which including each accident. As a result, the system can detect all accidents within 10 seconds. The more important point is that the detection capacity of odts could be enhanced automatically without any changes in the program codes as the training dataset becomes rich. 

Keywords— faster r-cnn for object detection, object tracking algorithm, object detection and tracking system, detection for unexpected events, tunnel cctv accident detection system},
  doi        = {10.1109/deep-ml.2019.00010},
  file       = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/10.1109@Deep-ML.2019.00010.pdf:PDF},
  groups     = {CNN, Applications, Easy},
  readstatus = {read},
}

@Conference{Obeidat2019,
  author     = {Raghad Obeidat and Rehab Duwairi and Ahmad Al-Aiad},
  booktitle  = {2019 International Conference on Deep Learning and Machine Learning in Emerging Applications (Deep-{ML})},
  title      = {A Collaborative Recommendation System for Online Courses Recommendations},
  year       = {2019},
  month      = {aug},
  publisher  = {{IEEE}},
  abstract   = {In this paper, we present a collaborative recommender system that recommends online courses for students based on similarities of students course history. The proposed system employs data mining techniques to discover patterns between courses. Consequently, we have noticed that clustering students into similar groups based on their respective course selections play a vital role in generating association rules of high quality when compared with the association rules generated using the whole set of courses and students. In particular, the apriori algorithm was used to generate association rules; Once using the whole dataset and once using the clusters which are formed based of the rules generated on clusters are better. Also, to assess the effect of course dependency on recommendations, we applied the spade algorithm on course sequences. The results are in harmony with the results obtained when apriori was applied. 

Keywords— course recommendation system, collaborative filtering, data mining, association rule mining},
  doi        = {10.1109/deep-ml.2019.00018},
  file       = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/Deep-ML.2019.00018.pdf:PDF},
  groups     = {Easy, CNN, Applications},
  readstatus = {read},
}

@Conference{Ay2019,
  author    = {Betul Ay and Galip Aydin and Zeynep Koyun and Mehmet Demir},
  booktitle = {2019 International Conference on Deep Learning and Machine Learning in Emerging Applications (Deep-{ML})},
  title     = {A Visual Similarity Recommendation System using Generative Adversarial Networks},
  year      = {2019},
  month     = {aug},
  publisher = {{IEEE}},
  abstract  = {The goal of content-based recommendation system is to retrieve and rank the list of items that are closest to the query item. Today, almost every e-commerce platform has a recommendation system strategy for products that customers can decide to buy. In this paper we describe our work on creating a generative adversarial network based image retrieval system for e-commerce platforms to retrieve best similar images for a given product image specifically for shoes. We compare state-of-the-art solutions and provide results for the proposed deep learning network on a standard data set. 

Keywords—image retrieval, deep learning, image similarity},
  doi       = {10.1109/deep-ml.2019.00017},
  file      = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/Deep-ML.2019.00017.pdf:PDF},
  groups    = {GAN, Applications},
}

@Conference{Kaskavalci2019,
  author    = {Halil Can Kaskavalci and Sezer Goren},
  booktitle = {2019 International Conference on Deep Learning and Machine Learning in Emerging Applications (Deep-{ML})},
  title     = {A Deep Learning Based Distributed Smart Surveillance Architecture using Edge and Cloud Computing},
  year      = {2019},
  month     = {aug},
  publisher = {{IEEE}},
  abstract  = {Smart surveillance is getting increasingly popular as technologies become easier to use and cheaper. Traditional surveillance records video footage to a storage device continuously. However, this generates enormous amount of data and reduces the life of the hard drive. Newer devices with internet connection save footage to the cloud. This feature comes with bandwidth requirements and extra cloud costs. In this paper, we propose a deep learning based, distributed, and scalable surveillance architecture using edge and cloud computing. Our design reduces both the bandwidth and as well as the cloud costs significantly by processing footage prior sending to the cloud. 

Index terms—surveillance, edge computing, cloud computing, internet of things, deep learning, smart gateway},
  doi       = {10.1109/deep-ml.2019.00009},
  file      = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/Deep-ML.2019.00009.pdf:PDF},
  groups    = {Cloud},
}

@Article{Roy2020,
  author    = {Subhankar Roy and Willi Menapace and Sebastiaan Oei and Ben Luijten and Enrico Fini and Cristiano Saltori and Iris Huijben and Nishith Chennakeshava and Federico Mento and Alessandro Sentelli and Emanuele Peschiera and Riccardo Trevisan and Giovanni Maschietto and Elena Torri and Riccardo Inchingolo and Andrea Smargiassi and Gino Soldati and Paolo Rota and Andrea Passerini and Ruud J. G. van Sloun and Elisa Ricci and Libertario Demi},
  journal   = {{IEEE} Transactions on Medical Imaging},
  title     = {Deep Learning for Classification and Localization of {COVID}-19 Markers in Point-of-Care Lung Ultrasound},
  year      = {2020},
  month     = {aug},
  number    = {8},
  pages     = {2676--2687},
  volume    = {39},
  abstract  = {Deep learning (dl) has proved successful in medical imaging and, in the wake of the recent covid- 19 pandemic, some works have started to investigate dlbased solutions for the assisted diagnosis of lung diseases. While existing works focus on ct scans, this paper studies the application of dl techniques for the analysis of lung ultrasonography (lus) images. Specifically, we present a novel fully-annotated dataset of lus images collected from several italian hospitals, with labels indicating the degree of disease severity at a frame-level, videolevel, and pixel-level (segmentation masks). Leveraging these data, we introduce several deep models that address relevant tasks for the automatic analysis of lus images. In particular, we present a novel deep network, derived from spatial transformer networks, which simultaneously predicts the disease severity score associated to a input frame and provides localization of pathological artefacts in a weakly-supervised way. Furthermore, we introduce a new method based on uninorms for effective frame score aggregation at a video-level. Finally, we benchmark state of the art deep models for estimating pixel-level segmentations of covid-19 imaging biomarkers. Experiments on the proposed dataset demonstrate satisfactory results on all the considered tasks, paving the way to future research on dl for the assisted diagnosis of covid-19 from lus data. 

Index terms—covid-19, lung ultrasound, deep learning},
  doi       = {10.1109/tmi.2020.2994459},
  file      = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/TMI.2020.2994459.pdf:PDF},
  groups    = {Applications, CNN},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Conference{Kalejahi2020,
  author    = {Behnam Kiani Kalejahi and Saeed Meshgini and Sabalan Daneshvar and Ali Farzamnia},
  booktitle = {2020 28th Iranian Conference on Electrical Engineering ({ICEE})},
  title     = {Bone Age Estimation by Deep Learning in X-Ray Medical Images},
  year      = {2020},
  month     = {aug},
  publisher = {{IEEE}},
  abstract  = {Patient skeletal age estimation using a skeletal bone age assessment method is a time consuming and very boring process. Today, in order to overcome these deficiencies, computerized techniques are used to replace hand-held techniques in the medical industry, to the extent that this results in better evaluation. The purpose of this research is to minimize the problems of the division of existing systems with deep learning algorithms and the high accuracy of diagnosis. The evaluation of skeletal bone age is the most clinical application for the study of endocrinology, genetic disorders and growth in young people. This assessment is usually performed using the radiologic analysis of the left wrist using the gp (greulich-pyle) technique or the tw(tanner-whitehouse) technique. Both techniques have many disadvantages, including a lack of human deductions from observations as well as being time-consuming. 

Keywords—diagnostic radiography, medical imaging, bone age assessment, deep learning},
  doi       = {10.1109/icee50131.2020.9260591},
  file      = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/kalejahi2020.pdf:PDF},
  groups    = {CNN, Applications},
}

@Conference{Hadjiyski2020,
  author    = {Nathan Hadjiyski},
  booktitle = {2020 International Conference on e-Health and Bioengineering ({EHB})},
  title     = {Kidney Cancer Staging: Deep Learning Neural Network Based Approach},
  year      = {2020},
  month     = {oct},
  publisher = {{IEEE}},
  abstract  = {Kidney cancer is a common type of cancer that can be very deadly. Proper cancer staging is critical for the correct selection of treatment for the affected patients. Stage 1 kidney cancer is an important threshold for the treatment decision. The physicians usually have difficulty to determine the cancer stage correctly, which can result in under- or overtreatment. Deep learning neural network (dlnn) was used to predict kidney cancer stage 1 versus higher stages in order to allow for a more accurate kidney cancer stage assessment by physicians. Computer tomography (ct) scans from 227 patients with different stages kidney cancer from the cancer imaging archive tcia database were used for training, validation, and testing of the dlnn. The kidney cancers were cropped from the 3d ct scans. The dataset was split into 48% training, 10% validation, and 42% test sets. Inception v3 dlnn with transfer learning was trained with the cropped kidney cancer training images. Area under the roc curve (auc) was used to estimate the classification accuracy. The auc of 0.97 for training, 0.91 for validation and 0.90 for the test sets was obtained. This ai system shows promise for potentially assisting physicians in kidney cancer staging. 

Keywords— kidney cancer staging, artificial intelligence (ai), deep learning neural network, computer tomography},
  doi       = {10.1109/ehb50910.2020.9280188},
  file      = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/hadjiyski2020.pdf:PDF},
  groups    = {CNN, Applications},
}

@Article{article,
  author   = {Saber, Mohamed and El-kenawy, El-Sayed},
  title    = {Design and implementation of accurate frequency estimator depend on deep learning},
  year     = {2020},
  month    = {07},
  abstract = {An Accurate, efficient, and stable system to estimate the unknown input frequency of a sinusoidal signal is presented. Thezproposed designzsolves the mainzdrawback of the existing phase-based estimator which called a derivative estimator depend on deep learning. These limi-tations are the inability to estimate low frequencies and the large estimation errors for the frequencies near the Nyquist rate. A Brief mathematical analysis in discrete-time of the proposed system is presented.ZProposedkestimator performance when the input is a singleksinusoid,kmultipleksinusoids in the presence ofkadditive whitekGaussian noise (AWGN) are provided. Thezaccuracy of the proposedzestimator is the result of dividing the dynamic range of estimation to three regions (low frequencies, middle frequencies, high frequencies) and specify a different formula to calculate the estimated frequency in each region. Thezboundaries of eachzregion are determined by using azGrey wolf optimizer (GWO) which training bidirectional recurrent neural networks (BRNN) to select the best weights for the estimated frequency. Thezsimulationzresults ensure thezaccuracy and validity of the proposed estimator compared to the traditional one. The hardware implementation of enhanced estimator using field-programmable gate array (FPGA), consumed 265 mW, and worked at 375 MHz.


Keywords: Frequency Estimation; Phase-Based Estimator; Deep Learning; FPGA; Neural Networks; GWO.},
  file     = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/4-Designandimplementationofaccuratefrequencyestimatordependondeeplearning.pdf:PDF},
  groups   = {Applications, Hard},
  url      = {https://www.researchgate.net/publication/342589443_Design_and_implementation_of_accurate_frequency_estimator_depend_on_deep_learning},
}

@Article{chen2021mogan,
  author        = {Jinshu Chen and Qihui Xu and Qi Kang and MengChu Zhou},
  title         = {MOGAN: Morphologic-structure-aware Generative Learning from a Single Image},
  year          = {2021},
  abstract      = {In most interactive image generation tasks, given regions of interest (roi) by users, the generated results are expected to have adequate diversities in appearance while maintaining correct and reasonable structures in original images. Such tasks become more challenging if only limited data is available. Recently proposed generative models complete training based on only one image. They pay much attention to the monolithic feature of the sample while ignoring the actual semantic information of different objects inside the sample. As a result, for roi-based generation tasks, they may produce inappropriate samples with excessive randomicity and without maintaining the related objects’ correct structures. To address this issue, this work introduces a morphologic-structure-aware generative adversarial network named mogan that produces random samples with diverse appearances and reliable structures based on only one image. For training for roi, we propose to utilize the data coming from the original image being augmented and bring in a novel module to transform such augmented data into knowledge containing both structures and appearances, thus enhancing the model’s comprehension of the sample. To learn the rest areas other than roi, we employ binary masks to ensure the generation isolated from roi. Finally, we set parallel and hierarchical branches of the mentioned learning process. Compared with other single image gan schemes, our approach focuses on internal features including the maintenance of rational structures and variation on appearance. Experiments confirm a better capacity of our model on roi-based image generation tasks than its competitive peers. 

Index terms—generative adversarial networks, single sample, morphologic awareness, roi-based image generation tasks},
  archiveprefix = {arXiv},
  eprint        = {2103.02997},
  file          = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/chen2021mogan - MOGAN_ Morphologic Structure Aware Generative Learning from a Single Image.pdf:PDF},
  groups        = {Hard, CNN},
  primaryclass  = {cs.CV},
  priority      = {prio3},
}

@Conference{9215249,
  author     = {A. K. {Venugopalan} and S. K. {Varughese} and S. M. {Jacob} and A. {George} and J. {Joseph}},
  booktitle  = {2020 International Conference on Smart Electronics and Communication (ICOSEC)},
  title      = {AI Based Audio Recognition System For Visually And Audibly Challenged},
  year       = {2020},
  pages      = {297-304},
  abstract   = {This work proposes a novel ai-based intelligent aiding system for visually challenged and partial audibly challenged people, who face difficulties in identifying the people with their voice, tracing the location during their travel, and find the direction towards a certain destination. The proposed system is designed to raise the amplitude of speaker in the bluetooth earphone to the required level for reducing the noise level to make it audible for the partially audible challenged people and give an alert in case of any emergency and send messages to their emergency contacts. 

Keywords – partially audibly challenged, visually challenged, artificial intelligence, deep learning, speaker identification, noise reduction, bluetooth earphone, mobile application},
  doi        = {10.1109/ICOSEC49089.2020.9215249},
  file       = {:C\:/Users/ACER/OneDrive/Documents/Jabref Reference/venugopalan2020.pdf:PDF},
  groups     = {RNN, Easy, Applications},
  owner      = {ACER},
  readstatus = {read},
  url        = {http://mufind.mu.edu.sa/EdsRecord/edseee,edseee.9215249/UserComments},
}

@Article{Redmon2016,
  author     = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  journal    = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title      = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
  year       = {2016},
  month      = may,
  note       = {arXiv: 1506.02640},
  abstract   = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  doi        = {10.1109/CVPR.2016.91},
  file       = {:Redmon2016 - You Only Look Once_ Unified, Real Time Object Detection.pdf:PDF},
  groups     = {CNN, Models, Easy},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  priority   = {prio1},
  shorttitle = {You {Only} {Look} {Once}},
  url        = {http://arxiv.org/abs/1506.02640},
  urldate    = {2021-04-07},
}

@Article{Ruder2017,
  author   = {Ruder, Sebastian},
  title    = {An overview of gradient descent optimization algorithms},
  year     = {2017},
  month    = jun,
  note     = {arXiv: 1609.04747},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  annote   = {Comment: Added derivations of AdaMax and Nadam},
  file     = {:Ruder2017 - An Overview of Gradient Descent Optimization Algorithms (1).pdf:PDF},
  groups   = {Easy, Models},
  keywords = {Computer Science - Machine Learning},
  url      = {http://arxiv.org/abs/1609.04747},
  urldate  = {2021-04-08},
}

@Article{Kingma2017,
  author     = {Kingma, Diederik P. and Ba, Jimmy},
  journal    = {International Conference for Learning Representations},
  title      = {Adam: {A} {Method} for {Stochastic} {Optimization}},
  year       = {2017},
  month      = jan,
  note       = {arXiv: 1412.6980 version: 9},
  abstract   = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  annote     = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
  file       = {:Kingma2017 - Adam_ a Method for Stochastic Optimization.pdf:PDF},
  groups     = {Models, Hard},
  keywords   = {Computer Science - Machine Learning},
  priority   = {prio1},
  shorttitle = {Adam},
  url        = {http://arxiv.org/abs/1412.6980},
  urldate    = {2021-04-08},
}

@Article{Farfade2015,
  author   = {Farfade, Sachin Sudhakar and Saberian, Mohammad and Li, Li-Jia},
  title    = {Multi-view {Face} {Detection} {Using} {Deep} {Convolutional} {Neural} {Networks}},
  year     = {2015},
  month    = apr,
  note     = {arXiv: 1502.02766},
  abstract = {In this paper we consider the problem of multi-view face detection. While there has been significant research on this problem, current state-of-the-art approaches for this task require annotation of facial landmarks, e.g. TSM [25], or annotation of face poses [28, 22]. They also require training dozens of models to fully capture faces in all orientations, e.g. 22 models in HeadHunter method [22]. In this paper we propose Deep Dense Face Detector (DDFD), a method that does not require pose/landmark annotation and is able to detect faces in a wide range of orientations using a single model based on deep convolutional neural networks. The proposed method has minimal complexity; unlike other recent deep learning object detection methods [9], it does not require additional components such as segmentation, bounding-box regression, or SVM classifiers. Furthermore, we analyzed scores of the proposed face detector for faces in different orientations and found that 1) the proposed method is able to detect faces from different angles and can handle occlusion to some extent, 2) there seems to be a correlation between dis- tribution of positive examples in the training set and scores of the proposed face detector. The latter suggests that the proposed methods performance can be further improved by using better sampling strategies and more sophisticated data augmentation techniques. Evaluations on popular face detection benchmark datasets show that our single-model face detector algorithm has similar or better performance compared to the previous methods, which are more complex and require annotations of either different poses or facial landmarks.},
  annote   = {Comment: in International Conference on Multimedia Retrieval 2015 (ICMR)},
  file     = {:Farfade2015 - Multi View Face Detection Using Deep Convolutional Neural Networks.pdf:PDF},
  groups   = {Applications, Easy},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4},
  url      = {http://arxiv.org/abs/1502.02766},
  urldate  = {2021-04-08},
}

@Article{Gatys2015,
  author   = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  journal  = {Journal of Vision},
  title    = {A {Neural} {Algorithm} of {Artistic} {Style}},
  year     = {2015},
  month    = sep,
  note     = {arXiv: 1508.06576},
  abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  doi      = {https://doi.org/10.1167/16.12.326},
  file     = {:Gatys2015 - A Neural Algorithm of Artistic Style.pdf:PDF},
  groups   = {Easy, CNN},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
  url      = {http://arxiv.org/abs/1508.06576},
  urldate  = {2021-04-08},
}

@Article{Girshick2015,
  author   = {Girshick, Ross},
  journal  = {IEEE International Conference on Computer Vision (ICCV)},
  title    = {Fast {R}-{CNN}},
  year     = {2015},
  month    = sep,
  note     = {arXiv: 1504.08083},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  annote   = {Comment: To appear in ICCV 2015},
  doi      = {10.1109/ICCV.2015.169},
  file     = {:Girshick2015 - Fast R CNN.pdf:PDF},
  groups   = {CNN, Hard},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  priority = {prio2},
  url      = {http://arxiv.org/abs/1504.08083},
  urldate  = {2021-04-08},
}

@Article{Ioffe2015,
  author     = {Ioffe, Sergey and Szegedy, Christian},
  journal    = {International Conference on International Conference on Machine Learning},
  title      = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
  year       = {2015},
  month      = mar,
  note       = {arXiv: 1502.03167},
  abstract   = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  file       = {:Ioffe2015 - Batch Normalization_ Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:PDF},
  groups     = {Hard, Models},
  keywords   = {Computer Science - Machine Learning},
  priority   = {prio1},
  shorttitle = {Batch {Normalization}},
  url        = {http://arxiv.org/abs/1502.03167},
  urldate    = {2021-04-08},
}

@Conference{AliQureshi2014,
  author     = {Ali Qureshi, M. and Deriche, M.},
  booktitle  = {2014 {IEEE} 11th {International} {Multi}-{Conference} on {Systems}, {Signals} {Devices} ({SSD14})},
  title      = {A review on copy move image forgery detection techniques},
  year       = {2014},
  month      = feb,
  pages      = {1--5},
  abstract   = {With the advent of powerful image editing tools, manipulating images and changing their content is becoming a trivial task. It is now possible to add, modify, or remove important features from an image without leaving any perceptual traces of tampering. With more than several million pictures uploaded daily to the net, and the introduction of e-Government services, it is becoming important to develop robust detection methods to identify image tampering operations. To this end, image forensics techniques aim at restoring trust and acceptance in digital media by uncovering tampering methods. Such detection techniques are the focus of this paper. In particular, we provide a survey of different forging detection techniques with a focus on copy and move approaches.},
  doi        = {10.1109/SSD.2014.6808907},
  file       = {:aliqureshi2014.pdf:PDF},
  groups     = {Project Base, Easy},
  keywords   = {Robustness, Image restoration, Image coding, Digital images, Physics, Quantization (signal), Arrays},
  priority   = {prio2},
  readstatus = {read},
}

@Conference{Abidin2019,
  author     = {Abidin, Arfa Binti Zainal and Majid, Hairudin Bin Abdul and Samah, Azurah Binti A and Hashim, Haslina Binti},
  booktitle  = {2019 6th {International} {Conference} on {Research} and {Innovation} in {Information} {Systems} ({ICRIIS})},
  title      = {Copy-{Move} {Image} {Forgery} {Detection} {Using} {Deep} {Learning} {Methods}: {A} {Review}},
  year       = {2019},
  month      = dec,
  note       = {ISSN: 2324-8157},
  pages      = {1--6},
  abstract   = {In recent years, the manipulation of digital images can be done with relative ease. This can be attributed to the technological advancement in the field of computing specifically with advanced, sophisticated image editing tool software. A majority of these software are user-friendly which results in its widespread use. However, this also presents a new problem where anyone with access to the software can easily manipulate an image and can use it for nefarious purposes such as spreading fake news. Due to this development of sophistication of tools and software like Adobe Photoshop, Pixir, and Affinity, digital images content is often simply manipulated and thus forged images are produced. Therefore, the process authenticating a digital image becomes difficult such as to distinguish between manipulated images and actual images through the naked eyes. Therefore, the importance of digital image forensics has attracted many researchers who are deeply involved in this area and has established many techniques for forgery detection in image forensics. Recently, deep learning approach has a high interest among researchers across the field and has shown good result in its application. Thus, forensic researchers attempt to apply deep learning approach as a method for detecting forgery image. This paper presents the understanding and extensive literature review of state-of-the-art techniques of deep learning in the detection of copy-move image forgery.},
  doi        = {10.1109/ICRIIS48246.2019.9073569},
  file       = {:10.1109@ICRIIS48246.2019.9073569.pdf:PDF},
  groups     = {Project Base, Easy, CNN},
  issn       = {2324-8157},
  keywords   = {Forgery, Digital images, Machine learning, Forensics, Feature extraction, Software, Watermarking, digital image forensics, copy-move forgery detection (CMFD), deep learning},
  priority   = {prio1},
  readstatus = {read},
  shorttitle = {Copy-{Move} {Image} {Forgery} {Detection} {Using} {Deep} {Learning} {Methods}},
}

@Article{Wang2019,
  author    = {Wang, Xinyi and Wang, He and Niu, Shaozhang and Zhang, Jiwei and Wang, Xinyi and Wang, He and Niu, Shaozhang and Zhang, Jiwei},
  journal   = {Mathematical Biosciences and Engineering},
  title     = {Detection and localization of image forgeries using improved mask regional convolutional neural network},
  year      = {2019},
  issn      = {1551-0018},
  number    = {5},
  pages     = {4581--4593},
  volume    = {16},
  abstract  = {The research on forgery detection and localization is significant in digital forensics and has attracted increasing attention recently. Traditional methods mostly use handcrafted or shallow-learning based features, but they have limited description ability and heavy computational costs. Recently, deep neural networks have shown to be capable of extracting complex statistical features from high-dimensional inputs and efficiently learning their hierarchical representations. In order to capture more discriminative features between tampered and non-tampered regions, we propose an improved mask regional convolutional neural network (Mask R-CNN) which attach a Sobel filter to the mask branch of Mask R-CNN in this paper. The Sobel filter acts as an auxiliary task to encourage predicted masks to have similar image gradients to the groundtruth mask. The overall network is capable of detecting two different types of image manipulations, including copy-move and splicing. The experimental results on two standard datasets show that the proposed model outperforms some state-of-the-art methods.},
  copyright = {2019 The Author(s)},
  doi       = {10.3934/mbe.2019229},
  file      = {:10.3934_mbe.2019229 (1).pdf:PDF},
  groups    = {Project Base, CNN, Applications},
  language  = {en},
  url       = {http://www.aimspress.com/rticle/doi/10.3934/mbe.2019229},
  urldate   = {2021-06-14},
}

@Article{Hashmi2014,
  author   = {Hashmi, Mohammad Farukh and Anand, Vijay and Keskar, Avinas G.},
  journal  = {AASRI Procedia},
  title    = {Copy-move {Image} {Forgery} {Detection} {Using} an {Efficient} and {Robust} {Method} {Combining} {Un}-decimated {Wavelet} {Transform} and {Scale} {Invariant} {Feature} {Transform}},
  year     = {2014},
  issn     = {2212-6716},
  month    = jan,
  pages    = {84--91},
  volume   = {9},
  abstract = {In the present digital world, digital images and videos are the main carrier of information. However, these sources of information can be easily tampered by using readily available software thus making authenticity and integrity of the digital images an important issue of concern. And in most of the cases copy- move image forgery is used to tamper the digital images. Therefore, as a solution to the aforementioned problem we are going to propose a unique method for copy-move forgery detection which can sustained various pre-processing attacks using a combination of Dyadic Wavelet Transform (DyWT) and Scale Invariant Feature Transform (SIFT). In this process first DyWT is applied on a given image to decompose it into four parts LL, LH, HL, and HH. Since LL part contains most of the information, we intended to apply SIFT on LL part only to extract the key features and find a descriptor vector of these key features and then find similarities between various descriptors vector to conclude that there has been some copy-move tampering done to the given image. And by using DyWT with SIFT we are able to extract more numbers of key points that are matched and thus able to detect copy-move forgery more efficiently.},
  doi      = {10.1016/j.aasri.2014.09.015},
  file     = {:Hashmi2014 - Copy Move Image Forgery Detection Using an Efficient and Robust Method Combining Un Decimated Wavelet Transform and Scale Invariant Feature Transform.pdf:PDF},
  groups   = {Project Base, Applications},
  keywords = {Digital Image Forgery, DyWT (Dyadic Wavelet Transform), SIFT (Scale Invariant Feature Transfrom).},
  language = {en},
  series   = {2014 {AASRI} {Conference} on {Circuit} and {Signal} {Processing} ({CSP} 2014)},
  url      = {https://www.sciencedirect.com/science/article/pii/S2212671614001152},
  urldate  = {2021-06-14},
}

@Article{Chen2019,
  author     = {Chen, Chien-Chang and Lu, Wei-Yu and Chou, Chung-Hsuan},
  journal    = {Multimedia Tools and Applications},
  title      = {Rotational copy-move forgery detection using {SIFT} and region growing strategies},
  year       = {2019},
  issn       = {1573-7721},
  month      = jul,
  number     = {13},
  pages      = {18293--18308},
  volume     = {78},
  abstract   = {The proposed scheme detects the copy–move forgery regions using SIFT, invariant moments calculation, and the region growing strategy. First, the SIFT-based keypoints are acquired as the significant features of an image. Second, pairs of keypoints with closed scales are examined to identify all possible pair blocks of the copy–move regions. Third, the orientations for each pair of matched keypoints are adjusted to have identical orientation. Lastly, the copy–move regions are acquired using the region growing technique, and invariant moment features are applied to each pair of matched blocks. Experimental results show that the proposed scheme efficiently and effectively detects rotational copy-move duplicated regions. Moreover, the proposed computation time is proportional to the number of keypoints and the size of the copy–move forgery regions.},
  doi        = {10.1007/s11042-019-7165-8},
  file       = {:10.1007@s11042-019-7165-8.pdf:PDF},
  groups     = {Project Base, Hard},
  language   = {en},
  priority   = {prio1},
  readstatus = {read},
  url        = {https://doi.org/10.1007/s11042-019-7165-8},
  urldate    = {2021-06-16},
}

@Article{Amerini2011,
  author   = {Amerini, Irene and Ballan, Lamberto and Caldelli, Roberto and Del Bimbo, Alberto and Serra, Giuseppe},
  journal  = {IEEE Transactions on Information Forensics and Security},
  title    = {A {SIFT}-{Based} {Forensic} {Method} for {Copy}–{Move} {Attack} {Detection} and {Transformation} {Recovery}},
  year     = {2011},
  issn     = {1556-6021},
  month    = sep,
  number   = {3},
  pages    = {1099--1110},
  volume   = {6},
  abstract = {One of the principal problems in image forensics is determining if a particular image is authentic or not. This can be a crucial task when images are used as basic evidence to influence judgment like, for example, in a court of law. To carry out such forensic analysis, various technological instruments have been developed in the literature. In this paper, the problem of detecting if an image has been forged is investigated; in particular, attention has been paid to the case in which an area of an image is copied and then pasted onto another zone to create a duplication or to cancel something that was awkward. Generally, to adapt the image patch to the new context a geometric transformation is needed. To detect such modifications, a novel methodology based on scale invariant features transform (SIFT) is proposed. Such a method allows us to both understand if a copy-move attack has occurred and, furthermore, to recover the geometric transformation used to perform cloning. Extensive experimental results are presented to confirm that the technique is able to precisely individuate the altered area and, in addition, to estimate the geometric transformation parameters with high reliability. The method also deals with multiple cloning.},
  doi      = {10.1109/TIFS.2011.2129512},
  file     = {:amerini2011.pdf:PDF},
  groups   = {Project Base, Hard},
  keywords = {Forensics, Forgery, Cloning, Feature extraction, Digital images, Discrete cosine transforms, Authenticity verification, copy–move attack, digital image forensics, geometric transformation recovery},
}

@InProceedings{Studiawan2021,
  author    = {Studiawan, Hudan and Salimi, Rahmat Nazali and Ahmad, Tohari},
  booktitle = {Proceedings of the 12th {International} {Conference} on {Soft} {Computing} and {Pattern} {Recognition} ({SoCPaR} 2020)},
  title     = {Forensic {Analysis} of {Copy}-{Move} {Attack} with {Robust} {Duplication} {Detection}},
  year      = {2021},
  address   = {Cham},
  editor    = {Abraham, Ajith and Ohsawa, Yukio and Gandhi, Niketa and Jabbar, M.A. and Haqiq, Abdelkrim and McLoone, Seán and Issac, Biju},
  pages     = {404--413},
  publisher = {Springer International Publishing},
  series    = {Advances in {Intelligent} {Systems} and {Computing}},
  abstract  = {Copy-move is one type of attack to forge a digital image where the attacker duplicates several areas of the image and paste them in different places to conceal a particular object on the original image. After the forgery, advanced methods such as noise addition and blurring, are often performed in the forged image to make it more challenging to recognize the attack. Therefore, it is required to do a preprocessing before conducting the detection. The preprocessing can be eliminated using a copy-move detection that is more resistant to noise addition and blurring. This paper proposes a new, flexible, and robust method that perform forensic analysis of both regular and advanced copy-move using modification and addition from two methods. The first method is designed to identify a regular copy-move attack, while the second one is effective for an advanced attack. The proposed method combines these two methods, can adapt to the forged image condition, and no preprocessing is required.},
  doi       = {10.1007/978-3-030-73689-7_39},
  file      = {:Studiawan2021 - Forensic Analysis of Copy Move Attack with Robust Duplication Detection.pdf:PDF},
  groups    = {Project Base},
  isbn      = {9783030736897},
  keywords  = {Copy-move attack , Image forensic , Duplication detection },
  language  = {en},
}

@Article{Agarwal2020,
  author   = {Agarwal, Ritu and Verma, Om Prakash},
  journal  = {Multimedia Tools and Applications},
  title    = {An efficient copy move forgery detection using deep learning feature extraction and matching algorithm},
  year     = {2020},
  issn     = {1573-7721},
  month    = mar,
  number   = {11},
  pages    = {7355--7376},
  volume   = {79},
  abstract = {The image forgery activities are on the rise because of the development of various image editing tools. Such activities are done by attackers with intentions of defaming people and websites or for gaining monetary advantage, extortion etc. Image forgeries are carried out through various ways, among one is the copy-move forgery. The basic process of copy-move image forgery is copying the objects present in an image and create the new image by using the copied objects or placing the copied object on the same image on a different location, hence the need for a forgery detection system to protect the authenticity of images. The existing forgery detection techniques detect the tampered regions with less efficiency because of the large size and lower contrast of the images. This article proposes an efficient technique for detecting the copy-move forged image based on deep learning. The proposed algorithm initializes the tampered image as the input for our system to detect the tampered region. Our system includes processes like segmentation, feature extraction, dense depth reconstruction, and finally identifying the tampered areas. The proposed deep learning based system can save on computational time and detect the duplicated regions with more accuracy.},
  doi      = {10.1007/s11042-019-08495-z},
  file     = {:Agarwal-Verma2020_Article_AnEfficientCopyMoveForgeryDete.pdf:PDF},
  groups   = {Project Base, CNN},
  language = {en},
  url      = {https://doi.org/10.1007/s11042-019-08495-z},
  urldate  = {2021-07-30},
}

@Article{ArmasVega2021,
  author   = {Armas Vega, Esteban Alejandro and González Fernández, Edgar and Sandoval Orozco, Ana Lucila and García Villalba, Luis Javier},
  journal  = {Neural Computing and Applications},
  title    = {Copy-move forgery detection technique based on discrete cosine transform blocks features},
  year     = {2021},
  issn     = {1433-3058},
  month    = may,
  number   = {10},
  pages    = {4713--4727},
  volume   = {33},
  abstract = {With the increasing number of software applications that allow altering digital images and their ease of use, they weaken the credibility of an image. This problem, together with the ease of distributing information through the Internet (blogs, social networks, etc.), has led to a tendency for information to be accepted as true without its veracity being questioned. Image counterfeiting has become a major threat to the credibility of the information. To deal with this threat, forensic image analysis is aimed at detecting and locating image forgeries using multiple clues that allows it to determine the veracity or otherwise of an image. In this paper, we present a method for the authentication of images. The proposed method performs detection of copy-move alterations within an image, using the discrete cosine transform. The characteristics obtained from these coefficients allow us to obtain transfer vectors, which are grouped together. Through the use of a tolerance threshold, it is possible to determine whether there are regions copied and pasted within the analysed image. The results obtained from the experiments reported in this paper demonstrate the effectiveness of the proposed method. For the evaluation of the proposed methods, experiments were carried out with public databases of falsified images that are widely used in the literature.},
  doi      = {10.1007/s00521-020-05433-1},
  file     = {:ArmasVega2021 - Copy Move Forgery Detection Technique Based on Discrete Cosine Transform Blocks Features.pdf:PDF},
  groups   = {Project Base},
  language = {en},
  url      = {https://doi.org/10.1007/s00521-020-05433-1},
  urldate  = {2021-07-30},
}

@Article{Pavlovic2019,
  author   = {Pavlovic, Aleksandra and Glišovic, Natasa and Gavrovska, Ana and Reljin, Irini},
  journal  = {Multimedia Tools and Applications},
  title    = {Copy-move forgery detection based on multifractals},
  year     = {2019},
  issn     = {1573-7721},
  month    = aug,
  number   = {15},
  pages    = {20655--20678},
  volume   = {78},
  abstract = {Digital images and video are the basic media for communication nowadays. They are used as authenticated proofs or corroboratory evidence in different areas like: forensic studies, law enforcement, journalism and others. With development of software for editing digital images, it has become very easy to change image content, add or remove important information or even to make one image combining multiple images. Thus, the development of methods for such change detection has become very important. One of the most common methods is copy-move forgery detection (CMFD). Methods of this type include change detection that occur by copying a part of an image and pasting it to another location within the image. We propose new method for detection of such changes using certain multifractal parameters as characteristic features, as well as common statistical parameters. Before the analysis, images are divided into non-overlapping blocks of fixed dimensions. For each block, the characteristic features are calculated. In order to classify observed blocks, we used metaheuristic method and proposed new semi-metric function for similarity analysis between blocks. Simulation shows that the proposed method provides good results in terms of precision and recall, with low computational complexity.},
  doi      = {10.1007/s11042-019-7277-1},
  file     = {:Pavlovic2019 - Copy Move Forgery Detection Based on Multifractals.pdf:PDF},
  groups   = {Project Base},
  language = {en},
  url      = {https://doi.org/10.1007/s11042-019-7277-1},
  urldate  = {2021-07-30},
}

@Article{Li2015,
  author     = {Li, Jian and Li, Xiaolong and Yang, Bin and Sun, Xingming},
  journal    = {IEEE Transactions on Information Forensics and Security},
  title      = {Segmentation-{Based} {Image} {Copy}-{Move} {Forgery} {Detection} {Scheme}},
  year       = {2015},
  issn       = {1556-6021},
  month      = mar,
  number     = {3},
  pages      = {507--518},
  volume     = {10},
  abstract   = {In this paper, we propose a scheme to detect the copy-move forgery in an image, mainly by extracting the keypoints for comparison. The main difference to the traditional methods is that the proposed scheme first segments the test image into semantically independent patches prior to keypoint extraction. As a result, the copy-move regions can be detected by matching between these patches. The matching process consists of two stages. In the first stage, we find the suspicious pairs of patches that may contain copy-move forgery regions, and we roughly estimate an affine transform matrix. In the second stage, an Expectation-Maximization-based algorithm is designed to refine the estimated matrix and to confirm the existence of copy-move forgery. Experimental results prove the good performance of the proposed scheme via comparing it with the state-of-the-art schemes on the public databases.},
  doi        = {10.1109/TIFS.2014.2381872},
  file       = {:jianli2015.pdf:PDF},
  groups     = {Project Base, Applications, Hard},
  keywords   = {Transforms, Image segmentation, Estimation, Forgery, Robustness, Accuracy, Educational institutions, Copy-move forgery detection, image forensics, segmentation, Copy-move forgery detection, image forensics, segmentation},
  readstatus = {read},
}

@Article{Elaskily2020,
  author   = {Elaskily, Mohamed A. and Elnemr, Heba A. and Sedik, Ahmed and Dessouky, Mohamed M. and El Banby, Ghada M. and Elshakankiry, Osama A. and Khalaf, Ashraf A. M. and Aslan, Heba K. and Faragallah, Osama S. and Abd El-Samie, Fathi E.},
  journal  = {Multimedia Tools and Applications},
  title    = {A novel deep learning framework for copy-moveforgery detection in images},
  year     = {2020},
  issn     = {1573-7721},
  month    = jul,
  number   = {27},
  pages    = {19167--19192},
  volume   = {79},
  abstract = {In this era of technology, digital images turn out to be ubiquitous in a contemporary society and they can be generated and manipulated by a wide variety of hardware and software technologies. Copy-move forgery is considered as an image tampering technique that aims to generate manipulated tampered images by concealing unwanted objects or reproducing desirable objects within the same image. Therefore, image content authentication has become an essential demand. In this paper, an innovative design for automatic detection of copy-move forgery based on deep learning approaches is proposed. A Convolutional Neural Network (CNN) is specifically designed for Copy-Move Forgery Detection (CMFD). The CNN is exploited to learn hierarchical feature representations from input images, which are used for detecting the tampered and original images. The extensive experiments demonstrate that the proposed deep CMFD algorithm outperforms the traditional CMFD systems by a considerable margin on the three publicly accessible datasets: MICC-F220, MICC-F2000, and MICC-F600. Furthermore, the three datasets are incorporated and joined to the SATs-130 dataset to form new combinations of datasets. An accuracy of 100\% has been achieved for the four datasets. This proves the robustness of the proposed algorithm against a diversity of known attacks. For better evaluation, comparative results are included.},
  doi      = {10.1007/s11042-020-08751-7},
  file     = {:Elaskily2020_Article_ANovelDeepLearningFrameworkFor.pdf:PDF},
  groups   = {Project Base, CNN},
  language = {en},
  url      = {https://doi.org/10.1007/s11042-020-08751-7},
  urldate  = {2021-08-04},
}

@Article{Abdalla2019,
  author    = {Abdalla, Younis and Iqbal, Tariq and Shehata, Mohamed S.},
  journal   = {Symmetry},
  title     = {Convolutional {Neural} {Network} for {Copy}-{Move} {Forgery} {Detection}},
  year      = {2019},
  issn      = {2073-8994},
  month     = oct,
  number    = {10},
  volume    = {11},
  abstract  = {Digital image forgery is a growing problem due to the increase in readily-available technology that makes the process relatively easy. In response, several approaches have been developed for detecting digital forgeries. This paper proposes a novel scheme based on neural networks and deep learning, focusing on the convolutional neural network (CNN) architecture approach to enhance a copy-move forgery detection. The proposed approach employs a CNN architecture that incorporates pre-processing layers to give satisfactory results. In addition, the possibility of using this model for various copy-move forgery techniques is explained. The experiments show that the overall validation accuracy is 90\%, with a set iteration limit.},
  copyright = {cc\_by\_nc},
  file      = {:Abdalla2019 - Convolutional Neural Network for Copy Move Forgery Detection.pdf:PDF},
  groups    = {Project Base, CNN},
  language  = {en},
  publisher = {MDPI},
  url       = {https://doi.org/10.3390/sym11101280},
  urldate   = {2021-08-04},
}

@InProceedings{Rao2016,
  author    = {Rao, Yuan and Ni, Jiangqun},
  booktitle = {2016 {IEEE} {International} {Workshop} on {Information} {Forensics} and {Security} ({WIFS})},
  title     = {A deep learning approach to detection of splicing and copy-move forgeries in images},
  year      = {2016},
  month     = dec,
  note      = {ISSN: 2157-4774},
  pages     = {1--6},
  abstract  = {In this paper, we present a new image forgery detection method based on deep learning technique, which utilizes a convolutional neural network (CNN) to automatically learn hierarchical representations from the input RGB color images. The proposed CNN is specifically designed for image splicing and copy-move detection applications. Rather than a random strategy, the weights at the first layer of our network are initialized with the basic high-pass filter set used in calculation of residual maps in spatial rich model (SRM), which serves as a regularizer to efficiently suppress the effect of image contents and capture the subtle artifacts introduced by the tampering operations. The pre-trained CNN is used as patch descriptor to extract dense features from the test images, and a feature fusion technique is then explored to obtain the final discriminative features for SVM classification. The experimental results on several public datasets show that the proposed CNN based model outperforms some state-of-the-art methods.},
  doi       = {10.1109/WIFS.2016.7823911},
  file      = {:A Deep Learning Approach to Detection Forgeries in Images2016.pdf:PDF},
  groups    = {Project Base, CNN},
  issn      = {2157-4774},
  keywords  = {Feature extraction, Convolution, Splicing, Forgery, Support vector machines, Kernel, Machine learning},
}

@Article{Rao2020,
  author    = {Rao, Allu Venkateswara and Rao, Chanamallu Srinivasa and Cheruku, Dharma Raj},
  journal   = {International Journal of Advanced Science and Technology},
  title     = {{AN} {INNOVATIVE} {AND} {EFFICIENT} {DEEP} {LEARNING} {ALGORITHM} {FOR} {COPY} {MOVE} {FORGERY} {DETECTION} {IN} {DIGITAL} {IMAGES}},
  year      = {2020},
  issn      = {2005-4238},
  month     = apr,
  number    = {05},
  pages     = {10531--10542},
  volume    = {29},
  abstract  = {Digital images have become more easier to tamper with the rapid advancement in image processing tools and software. Digital image manipulation takes part to deform the content of a picture in order to accomplish some deceit purposes. Such deceits are acknowledged as forgeries.The forensic people  need an effective means of observing such maliciously tampered data. Some important and efficient forgeries are; image retouchingand,splicing and Copy-move forgery(CMF). Copy-move forgery is the most significant type of image forgery due to its effective nes and simplicity. CMF is nothing but the image copying from one location and pasting it in another location. In this process, we conceal the existing data in the picture or to generate a simulated image. This region duplication process modifies the meaning of the picture totally. The precise forgery detection plays a key role in digital images. Image forgery detection approaches may be active or passive. Copy-move forgery detection (CMFD) is a passive-Blind image forgery detection method. CMFD mainly impresses on the speed and rigorous of the detection method. The proposed CMFD method presents a new approach to solve the issues in existing methods that the tampered area is resized or rotated after attachment. The proposed method is an innovative and efficient algorithm called Generalized Approximate Reasoning-Based Intelligence Control (GARIC) algorithm. Hence, GARIC deep learning approcch is used to detect the presence of falcification in images.},
  copyright = {Copyright (c)},
  file      = {:Rao2020 - AN INNOVATIVE aND EFFICIENT DEEP LEARNING ALGORITHM fOR COPY MOVE FORGERY DETECTION iN DIGITAL IMAGES.pdf:PDF},
  groups    = {Project Base, CNN},
  language  = {en},
  url       = {http://sersc.org/journals/index.php/IJAST/article/view/24161},
  urldate   = {2021-08-04},
}

@Article{Liu2017,
  author   = {Liu, Yaqi and Guan, Qingxiao and Zhao, Xianfeng},
  journal  = {arXiv:1707.01221 [cs]},
  title    = {Copy-move {Forgery} {Detection} based on {Convolutional} {Kernel} {Network}},
  year     = {2017},
  month    = jul,
  note     = {arXiv: 1707.01221 version: 1},
  abstract = {In this paper, a copy-move forgery detection method based on Convolutional Kernel Network is proposed. Different from methods based on conventional hand-crafted features, Convolutional Kernel Network is a kind of data-driven local descriptor with the deep convolutional structure. Thanks to the development of deep learning theories and widely available datasets, the data-driven methods can achieve competitive performance on different conditions for its excellent discriminative capability. Besides, our Convolutional Kernel Network is reformulated as a series of matrix computations and convolutional operations which are easy to parallelize and accelerate by GPU, leading to high efficiency. Then, appropriate preprocessing and postprocessing for Convolutional Kernel Network are adopted to achieve copy-move forgery detection. Particularly, a segmentation-based keypoints distribution strategy is proposed and a GPU-based adaptive oversegmentation method is adopted. Numerous experiments are conducted to demonstrate the effectiveness and robustness of the GPU version of Convolutional Kernel Network, and the state-of-the-art performance of the proposed copy-move forgery detection method based on Convolutional Kernel Network.},
  annote   = {Comment: 26 pages, 8 figures, submitted to Multimedia Tools and Applications},
  file     = {:Liu2017 - Copy Move Forgery Detection Based on Convolutional Kernel Network.pdf:PDF},
  groups   = {Project Base, CNN},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
  url      = {http://arxiv.org/abs/1707.01221},
  urldate  = {2021-08-04},
}

@Article{Abdalla2019a,
  author    = {Abdalla, Younis and Iqbal, M. T. and Shehata, M.},
  journal   = {European Journal of Electrical Engineering and Computer Science},
  title     = {Image {Forgery} {Detection} {Based} on {Deep} {Transfer} {Learning}},
  year      = {2019},
  issn      = {2736-5751},
  month     = sep,
  number    = {5},
  volume    = {3},
  copyright = {Copyright (c) 2019 Younis Abdalla, M. T. Iqbal, M. Shehata},
  doi       = {10.24018/ejece.2019.3.5.125},
  file      = {:Abdalla2019a - Image Forgery Detection Based on Deep Transfer Learning.pdf:PDF},
  groups    = {Project Base, CNN},
  keywords  = {Forgery detection, Deep learning, Transfer learning, Neural network},
  language  = {en},
  url       = {https://www.ejece.org/index.php/ejece/article/view/125},
  urldate   = {2021-08-04},
}

@Article{Diallo2020,
  author   = {Diallo, Boubacar and Urruty, Thierry and Bourdon, Pascal and Fernandez-Maloigne, Christine},
  journal  = {Forensic Science International: Reports},
  title    = {Robust forgery detection for compressed images using {CNN} supervision},
  year     = {2020},
  issn     = {2665-9107},
  month    = dec,
  pages    = {100112},
  volume   = {2},
  abstract = {Images available on online sharing platforms have a high probability of being modified, with additional global transformations such as compression, resizing or filtering covering the possible alteration. Such manipulations impose many constraints on forgery detection algorithms. This article presents a framework improving robustness for image forgery detection. The most important step of our framework is to take into account the image quality corresponding to the chosen application. Therefore, we relied on a camera identification model based on convolutional neural networks. Lossy compression such as JPEG being considered as the most common type of intentional or inadvertent concealment of image forgery, that leads us to experiment our proposal on this manipulation. Thus, our trained CNN is fed with a mixture of different qualities of compressed and uncompressed images. Experimental results showed the importance of this step to improve the effectiveness of our approach against recent literature approaches. To better interpret our trained CNN, we proposed an in-depth supervision by first a visualization of the layer and an experimental analysis of the influence of the learned features. This analysis led us to a more robust and accurate framework. Finally, we applied this improved system on an image forgery detection application and showed some promising results.},
  doi      = {10.1016/j.fsir.2020.100112},
  file     = {:Diallo2020 - Robust Forgery Detection for Compressed Images Using CNN Supervision.pdf:PDF},
  groups   = {Project Base, CNN},
  keywords = {Forgery image detection, Compression, Camera model identification, Convolutional neural networks supervision},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S266591072030061X},
  urldate  = {2021-08-04},
}

@Article{Darmet2021,
  author   = {Darmet, Ludovic and Wang, Kai and Cayre, François},
  journal  = {Applied Soft Computing},
  title    = {Disentangling copy-moved source and target areas},
  year     = {2021},
  issn     = {1568-4946},
  month    = sep,
  pages    = {107536},
  volume   = {109},
  abstract = {Copy-move is a very popular image falsification where a semantically coherent part of the image, the source area, is copied and pasted at another position within the same image as the so-called target area. The majority of existing copy-move detectors search for matching areas and thus identify the source and target zones indifferently, while only the target really represents a tampered area. To the best of our knowledge, at the moment of preparing this paper there has been only one published method called BusterNet that is capable of performing source and target disambiguation by using a specifically designed deep neural network. Different from the deep-learning-based BusterNet method, we propose in this paper a source and target disentangling approach based on local statistical model of image patches. Our proposed method acts as a second-stage detector after a first stage of copy-move detection of duplicated areas. We had the following intuition: even if no manipulation (e.g., scaling and rotation) is added on target area, its boundaries should expose a statistical deviation from the pristine area and the source area; further, if the target area is manipulated, the deviation should appear not only on the boundaries but on the full zone. Our method relies on machine learning tool with Gaussian Mixture Model to describe likelihood of image patches. Likelihoods are then compared between the pristine region and the candidate source/target areas as identified by the first-stage detector. Experiments and comparisons demonstrate the effectiveness of the proposed method.},
  doi      = {10.1016/j.asoc.2021.107536},
  file     = {:preprint_asc.pdf:PDF},
  groups   = {Project Base, CNN},
  keywords = {Image forensics, Copy-move detection, Image statistics, Gaussian Mixture Model, Likelihood},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S1568494621004592},
  urldate  = {2021-08-04},
}

@InProceedings{Kafali2021,
  author     = {Kafali, Efthimia and Vretos, Nicholas and Semertzidis, Theodoros and Daras, Petros},
  booktitle  = {2020 25th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
  title      = {{RobusterNet}: {Improving} {Copy}-{Move} {Forgery} {Detection} with {Volterra}-based {Convolutions}},
  year       = {2021},
  month      = jan,
  note       = {ISSN: 1051-4651},
  pages      = {1160--1165},
  abstract   = {Convolutional Neural Networks (CNNs) have recently been introduced for addressing copy-move forgery detection (CMFD). However, current CMFD CNN-based approaches have insufficient performance commitment regarding the localization of the positive class. In this paper, this issue is explored by considering both linear and nonlinear interactions between pixels. A nonlinear Inception module based on second-order Volterra kernels is proposed, in order to ameliorate the results of a state-of-the-art CMFD architecture. The outcome of this work shows that a combination of linear and nonlinear convolution kernels can make the input foreground and background pixels more separable. The proposed approach is evaluated on CASIA and CoMoFoD, two publicly available CMFD datasets, and results to an improved positive class localization performance. Moreover, the findings of the proposed method imply that the nonlinear Inception module stimulates immense robustness against miscellaneous post processing attacks.},
  doi        = {10.1109/ICPR48806.2021.9412587},
  file       = {:RobusterNet_Improving_Copy-Move_Forgery_Detection_with_Volterra-based_Convolutions.pdf:PDF},
  groups     = {Project Base, CNN},
  issn       = {1051-4651},
  keywords   = {Location awareness, Image segmentation, Convolution, Feature extraction, Forgery, Robustness, Pattern recognition},
  shorttitle = {{RobusterNet}},
}

@Article{Isnanto2020,
  author    = {Isnanto, R. Rizal and Zahra, Ajub Ajulian and Santoso, Imam and Lubis, Muhammad Salman},
  journal   = {International Journal of Electronics and Telecommunications},
  title     = {Determination of the {Optimal} {Threshold} {Value} and {Number} of {Keypoints} in {Scale} {Invariant} {Feature} {Transform}-based {Copy}-{Move} {Forgery} {Detection}},
  year      = {2020},
  issn      = {2300-1933},
  month     = sep,
  number    = {3},
  pages     = {561--569},
  volume    = {66},
  abstract  = {The copy-move forgery detection (CMFD) begins with the preprocessing until the image is ready to process. Then, the image features are extracted using a feature-transform-based extraction called the scale-invariant feature transform (SIFT). The last step is features matching using Generalized 2 Nearest-Neighbor (G2NN) method with threshold values variation. The problem is what is the optimal threshold value and number of keypoints so that copy-move detection has the highest accuracy. The optimal threshold value and number of keypoints had determined so that the detection has the highest accuracy. The research was carried out on images without noise and with Gaussian noise.},
  copyright = {Copyright (c) 2020 International Journal of Electronics and Telecommunications},
  file      = {:Isnanto2020 - Determination of the Optimal Threshold Value and Number of Keypoints in Scale Invariant Feature Transform Based Copy Move Forgery Detection.pdf:PDF},
  groups    = {Project Base, CNN},
  language  = {en},
  url       = {http://ijet.pl/index.php/ijet/article/view/10.24425-ijet.2020.134013},
  urldate   = {2021-08-04},
}

@Misc{,
  title    = {A {ConvNet} {Based} {Procedure} for {Image} {Copy}-{Move} {Forgery} {Detection}},
  abstract = {Copy-Move forgery is one of the popular image tempering procedure. In which the forger modifies the original image by creating multiple instances of some objects within the image itself. Recently, several deep convnet methods have been applied in …},
  file     = {:A ConvNet Based Procedure for Image Copy-Move Forgery Detection _ SpringerLink.pdf:PDF},
  groups   = {Project Base, CNN},
  journal  = {springerprofessional.de},
  language = {en},
  url      = {https://www.springerprofessional.de/en/a-convnet-based-procedure-for-image-copy-move-forgery-detection/18113458},
  urldate  = {2021-08-04},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Project Base\;0\;1\;0xffffffff\;\;\;;
1 StaticGroup:Hard\;0\;1\;0xff0000ff\;\;Hard to understand\;;
1 StaticGroup:Easy\;0\;0\;0x4d804dff\;\;Easy to understand\;;
1 StaticGroup:Models\;0\;1\;0x1a3399ff\;\;Models\;;
2 StaticGroup:CNN\;0\;0\;0x4d3399ff\;\;CNN related papers\;;
2 StaticGroup:RNN\;0\;0\;0x80801aff\;\;RNN related papers\;;
2 StaticGroup:GAN\;0\;1\;0x4d8080ff\;\;GAN related papers\;;
1 StaticGroup:Cloud\;0\;0\;0x8a8a8aff\;\;Cloud\;;
1 StaticGroup:Applications\;0\;1\;0xa64ad1f5\;\;Applications of Deep Learning\;;
}
